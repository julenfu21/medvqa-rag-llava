{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA Model Result Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import disable_progress_bars\n",
    "\n",
    "import src.utils.dataset_helpers.world_med_qa_v.dataset_management as world_med_qa_v_dataset_management\n",
    "import src.utils.dataset_helpers.world_med_qa_v.plot_helpers as world_med_qa_v_plot_helpers\n",
    "from src.utils.data_definitions import DocSplitOptions\n",
    "from src.utils.enums import RagQPromptType, VQAStrategyType, ZeroShotPromptType\n",
    "from src.utils.string_formatting_helpers import to_snake_case_strategy_name\n",
    "from src.visual_qa_model import VisualQAModel\n",
    "from src.visual_qa_strategies.base_vqa_strategy import BaseVQAStrategy\n",
    "from src.visual_qa_strategies.rag_q_vqa_strategy import RagQVQAStrategy\n",
    "from src.visual_qa_strategies.zero_shot_vqa_strategy import ZeroShotVQAStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Configure Environment Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect Google Colab Form Annotation Automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "function code_toggle(id) {\n",
       "    var cells = document.querySelectorAll(\".jp-CodeCell\");\n",
       "    for (var cell of cells) {\n",
       "        if (cell.querySelector(\"#\" + id) !== null) {\n",
       "            var div = cell.querySelector(\".jp-InputArea\");\n",
       "            if (div.style.display === \"none\") {\n",
       "                div.style.display = \"block\";\n",
       "            } else {\n",
       "                div.style.display = \"none\";\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext ipyform\n",
    "%form_config --auto-detect 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable Automatic Module Reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable Progress Bar for Dataset Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_progress_bars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation of VQA Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(\"data/WorldMedQA-V\")\n",
    "MODEL_NAME = \"llava\"\n",
    "COUNTRY = \"spain\"\n",
    "FILE_TYPE = \"english\"\n",
    "RESULTS_DIR = Path('evaluation_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading WorldMedQA-V dataset (filename: spain_english_processed.tsv) ...\n",
      "+ WorldMedQA-V dataset (filename: spain_english_processed.tsv) loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'image', 'question', 'A', 'B', 'C', 'D', 'answer', 'correct_option', 'split'],\n",
       "    num_rows: 125\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_med_qa_v_dataset = world_med_qa_v_dataset_management.load_vqa_dataset(\n",
    "    data_path=DATASET_DIR,\n",
    "    country=COUNTRY,\n",
    "    file_type=FILE_TYPE\n",
    ")\n",
    "world_med_qa_v_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Zero-Shot Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading Zero-Shot strategy ...\n",
      "+ Zero-Shot strategy loaded.\n",
      "- Loading Llava model (prompt template: zs_v1) ...\n",
      "+ Llava model (prompt template: zs_v1) loaded.\n"
     ]
    }
   ],
   "source": [
    "llava_model = VisualQAModel(\n",
    "    visual_qa_strategy=ZeroShotVQAStrategy(prompt_type=ZeroShotPromptType.V1),\n",
    "    model_name=MODEL_NAME,\n",
    "    country=COUNTRY,\n",
    "    file_type=FILE_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (Prompt Template: `zs_v1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_model.evaluate(\n",
    "    dataset=world_med_qa_v_dataset.take(5),\n",
    "    save_path=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (Prompt Template: `zs_v2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Evaluating model (spain_english subset) ...: 100%|██████████| 5/5 [06:20<00:00, 76.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Model evaluation (spain_english subset) completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llava_model.visual_qa_strategy.prompt_type = ZeroShotPromptType.V2\n",
    "llava_model.evaluate(\n",
    "    dataset=world_med_qa_v_dataset.take(5),\n",
    "    save_path=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (Prompt Template: `zs_v3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Evaluating model (spain_english subset) ...: 100%|██████████| 5/5 [06:24<00:00, 76.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Model evaluation (spain_english subset) completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llava_model.visual_qa_strategy.prompt_type = ZeroShotPromptType.V3\n",
    "llava_model.evaluate(\n",
    "    dataset=world_med_qa_v_dataset.take(5),\n",
    "    save_path=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Retrieval-Augmented Generation (RAG) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model Specific Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_DIR = Path('data/WikiMed/indexed_db')\n",
    "INDEX_NAME = \"Wikimed+S-PubMedBert-MS-MARCO-FullTexts\"\n",
    "EMBEDDING_MODEL_NAME = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "RELEVANT_DOCS_COUNT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. RAG Q (Question Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading RAG Q strategy ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing required arguments: \n\t- relevant_docs_count: int \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llava_model\u001b[38;5;241m.\u001b[39mvisual_qa_strategy \u001b[38;5;241m=\u001b[39m \u001b[43mRagQVQAStrategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRagQPromptType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINDEX_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINDEX_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDING_MODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# relevant_docs_count=RELEVANT_DOCS_COUNT\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TFM/world-med-visual-qa/src/visual_qa_strategies/base_vqa_strategy.py:25\u001b[0m, in \u001b[0;36mBaseVQAStrategy.__init__\u001b[0;34m(self, prompt_type, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_strategy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m strategy ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_prompt_template()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformatted_strategy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m strategy loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/TFM/world-med-visual-qa/src/visual_qa_strategies/rag_q_vqa_strategy.py:36\u001b[0m, in \u001b[0;36mRagQVQAStrategy._init_strategy\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_strategy\u001b[39m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[1;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     arguments \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m         ArgumentSpec(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected_type\u001b[38;5;241m=\u001b[39mPath),\n\u001b[1;32m     32\u001b[0m         ArgumentSpec(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m),\n\u001b[1;32m     33\u001b[0m         ArgumentSpec(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_model_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m),\n\u001b[1;32m     34\u001b[0m         ArgumentSpec(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevant_docs_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, expected_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     35\u001b[0m     ]\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_arguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__retriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load_wikimed_retriever(\n\u001b[1;32m     39\u001b[0m         index_dir\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     40\u001b[0m         index_name\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_name\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     41\u001b[0m         embedding_model_name\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_model_name\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     42\u001b[0m         relevant_docs_count\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevant_docs_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/TFM/world-med-visual-qa/src/visual_qa_strategies/base_vqa_strategy.py:74\u001b[0m, in \u001b[0;36mBaseVQAStrategy._validate_arguments\u001b[0;34m(self, required_arguments, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m argument \u001b[38;5;129;01min\u001b[39;00m missing_arguments:\n\u001b[1;32m     73\u001b[0m     error_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margument\u001b[38;5;241m.\u001b[39mexpected_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing required arguments: \n\t- relevant_docs_count: int \n"
     ]
    }
   ],
   "source": [
    "llava_model.visual_qa_strategy = RagQVQAStrategy(\n",
    "    prompt_type=RagQPromptType.V1,\n",
    "    index_dir=INDEX_DIR,\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "    relevant_docs_count=RELEVANT_DOCS_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (Prompt Template: `rq_v1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Evaluating model (spain_english subset) ...:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected keyword arguments: patata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllava_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_med_qa_v_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRESULTS_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m123\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# chunk_size=500,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# chunk_overlap=0,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# short_docs_count=1\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TFM/world-med-visual-qa/src/visual_qa_model.py:179\u001b[0m, in \u001b[0;36mVisualQAModel.evaluate\u001b[0;34m(self, dataset, save_path, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m row_index \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    177\u001b[0m gold_options[row_index] \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_option\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 179\u001b[0m model_answer_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_answer_from_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m predicted_options[row_index] \u001b[38;5;241m=\u001b[39m model_answer_result\u001b[38;5;241m.\u001b[39manswer\n\u001b[1;32m    181\u001b[0m current_relevant_documents \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/TFM/world-med-visual-qa/src/visual_qa_model.py:63\u001b[0m, in \u001b[0;36mVisualQAModel.generate_answer_from_row\u001b[0;34m(self, row, possible_options, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Generating Answer for Question (ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m model_answer_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__visual_qa_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_answer_from_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpossible_answers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43moption\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43moption\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossible_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase64_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+ Answer for Question (ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) generated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/TFM/world-med-visual-qa/src/visual_qa_strategies/rag_q_vqa_strategy.py:108\u001b[0m, in \u001b[0;36mRagQVQAStrategy.generate_answer_from_row\u001b[0;34m(self, model, question, possible_answers, base64_image, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer_from_row\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    102\u001b[0m     model: BaseChatModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[1;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelAnswerResult:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_arguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequired_arguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mArgumentSpec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdoc_split_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                \u001b[49m\u001b[43mexpected_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDocSplitOptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                \u001b[49m\u001b[43mis_optional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_docs\u001b[39m(docs: \u001b[38;5;28mlist\u001b[39m[Document], split_options: DocSplitOptions) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[1;32m    120\u001b[0m         text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m    121\u001b[0m             chunk_size\u001b[38;5;241m=\u001b[39msplit_options\u001b[38;5;241m.\u001b[39mchunk_size,\n\u001b[1;32m    122\u001b[0m             chunk_overlap\u001b[38;5;241m=\u001b[39msplit_options\u001b[38;5;241m.\u001b[39mchunk_overlap\n\u001b[1;32m    123\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/TFM/world-med-visual-qa/src/visual_qa_strategies/base_vqa_strategy.py:51\u001b[0m, in \u001b[0;36mBaseVQAStrategy._validate_arguments\u001b[0;34m(self, required_arguments, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m extra_arguments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m required_arguments_names\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_arguments:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(extra_arguments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     55\u001b[0m missing_arguments \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m argument \u001b[38;5;129;01min\u001b[39;00m required_arguments:\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected keyword arguments: patata"
     ]
    }
   ],
   "source": [
    "llava_model.evaluate(\n",
    "    dataset=world_med_qa_v_dataset.take(5),\n",
    "    save_path=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (Prompt Template: `rq_v2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Evaluating model (spain_english subset) ...: 100%|██████████| 5/5 [06:52<00:00, 82.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Model evaluation (spain_english subset) completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llava_model.visual_qa_strategy.prompt_type = RagQPromptType.V2\n",
    "llava_model.evaluate(\n",
    "    dataset=world_med_qa_v_dataset.take(5),\n",
    "    save_path=RESULTS_DIR,\n",
    "    doc_split_options = DocSplitOptions(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=0,\n",
    "        short_docs_count=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (Prompt Template: `rq_v3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RagQPromptType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llava_model\u001b[38;5;241m.\u001b[39mvisual_qa_strategy\u001b[38;5;241m.\u001b[39mprompt_type \u001b[38;5;241m=\u001b[39m \u001b[43mRagQPromptType\u001b[49m\u001b[38;5;241m.\u001b[39mV3\n\u001b[1;32m      2\u001b[0m llava_model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m      3\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mworld_med_qa_v_dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m      4\u001b[0m     save_path\u001b[38;5;241m=\u001b[39mRESULTS_DIR,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     short_docs_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RagQPromptType' is not defined"
     ]
    }
   ],
   "source": [
    "llava_model.visual_qa_strategy.prompt_type = RagQPromptType.V3\n",
    "llava_model.evaluate(\n",
    "    dataset=world_med_qa_v_dataset.take(5),\n",
    "    save_path=RESULTS_DIR,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0,\n",
    "    short_docs_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (Prompt Template: `rq_v4`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Evaluating model (spain_english subset) ...: 100%|██████████| 5/5 [16:48<00:00, 201.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Model evaluation (spain_english subset) completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llava_model.visual_qa_strategy.prompt_type = RagQPromptType.V4\n",
    "llava_model.evaluate(\n",
    "    dataset=world_med_qa_v_dataset.take(5),\n",
    "    save_path=RESULTS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. RAG Q+As (Question + Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. RAG IMG (Image-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4. RAG DB-Reranker (Database with Reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VQA Approaches Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model Specific Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(\"data/WorldMedQA-V\")\n",
    "MODEL_NAME = \"llava\"\n",
    "COUNTRY = \"spain\"\n",
    "FILE_TYPE = \"english\"\n",
    "RESULTS_DIR = Path('evaluation_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define RAG Q Specific Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_DIR = Path('data/WikiMed/indexed_db')\n",
    "INDEX_NAME = \"Wikimed+S-PubMedBert-MS-MARCO-FullTexts\"\n",
    "EMBEDDING_MODEL_NAME = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "RELEVANT_DOCS_COUNT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Possible VQA Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading Zero-Shot strategy ...\n",
      "+ Zero-Shot strategy loaded.\n",
      "- Loading RAG Q strategy ...\n",
      "\t- Loading Embeddings ...\n",
      "\t+ Embeddings Loaded.\n",
      "\t- Loading Index ...\n",
      "\t+ Index Loaded.\n",
      "\t- Loading Retriever ...\n",
      "\t+ Retriever Loaded.\n",
      "+ RAG Q strategy loaded.\n"
     ]
    }
   ],
   "source": [
    "vqa_strategies: dict[VQAStrategyType, BaseVQAStrategy] = {\n",
    "    VQAStrategyType.ZERO_SHOT: ZeroShotVQAStrategy(prompt_type=ZeroShotPromptType.V1),\n",
    "    VQAStrategyType.RAG_Q: RagQVQAStrategy(\n",
    "        prompt_type=RagQPromptType.V1,\n",
    "        index_dir=INDEX_DIR,\n",
    "        index_name=INDEX_NAME,\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        relevant_docs_count=RELEVANT_DOCS_COUNT\n",
    "    ),\n",
    "    VQAStrategyType.RAG_Q_AS: None,\n",
    "    VQAStrategyType.RAG_IMG: None,\n",
    "    VQAStrategyType.RAG_DB_RERANKER: None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loading WorldMedQA-V dataset (filename: spain_english_processed.tsv) ...\n",
      "+ WorldMedQA-V dataset (filename: spain_english_processed.tsv) loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'image', 'question', 'A', 'B', 'C', 'D', 'answer', 'correct_option', 'split'],\n",
       "    num_rows: 125\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_med_qa_v_dataset = world_med_qa_v_dataset_management.load_vqa_dataset(\n",
    "    data_path=DATASET_DIR,\n",
    "    country=COUNTRY,\n",
    "    file_type=FILE_TYPE\n",
    ")\n",
    "world_med_qa_v_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2495cf5a7b74fa7abbd89f52b7e7079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FormWidget(children=(VBox(children=(HTML(value=''), HTML(value='<h2>Interactive VQA Model Exploration Form</h2…"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Interactive VQA Model Exploration Form\n",
    "vqa_strategy_type = 'Zero-Shot' # @param [\"Zero-Shot\", \"RAG Q\", \"RAG Q+As\", \"RAG IMG\", \"RAG DB-Reranker\"]\n",
    "prompt_type = \"zs_v1\" # @param [\"zs_v1\", \"zs_v2\", \"zs_v3\", \"rq_v1\", \"rq_v2\", \"rq_v3\", \"rq_v4\"]\n",
    "question_id = 1 # @param {\"type\":\"integer\"}\n",
    "image_width = 600 # @param {\"type\":\"integer\"}\n",
    "action = 'Fetch from JSON' # @param [\"Execute Model\", \"Fetch from JSON\"]\n",
    "\n",
    "\n",
    "row = world_med_qa_v_dataset_management.get_dataset_row_by_id(\n",
    "    dataset=world_med_qa_v_dataset,\n",
    "    question_id=question_id\n",
    ")\n",
    "\n",
    "if action == \"Execute Model\":\n",
    "    formatted_vqa_strategy_type = to_snake_case_strategy_name(strategy_name=vqa_strategy_type)\n",
    "    chosen_vqa_strategy = vqa_strategies[VQAStrategyType(formatted_vqa_strategy_type)]\n",
    "    chosen_vqa_strategy.prompt_type = ZeroShotPromptType(prompt_type)\n",
    "    model=VisualQAModel(\n",
    "        visual_qa_strategy=chosen_vqa_strategy,\n",
    "        model_name=MODEL_NAME,\n",
    "        country=COUNTRY,\n",
    "        file_type=FILE_TYPE\n",
    "    )\n",
    "    world_med_qa_v_plot_helpers.visualize_qa_pair_row(\n",
    "        row=row,\n",
    "        image_width=image_width,\n",
    "        model_answer=model.generate_answer_from_row(\n",
    "            row=row,\n",
    "            possible_options=['A', 'B', 'C', 'D'],\n",
    "            verbose=True\n",
    "        )\n",
    "    )\n",
    "elif action == \"Fetch from JSON\":\n",
    "    model_answer = world_med_qa_v_dataset_management.fetch_model_answer_from_json(\n",
    "        evaluation_results_folder=RESULTS_DIR,\n",
    "        vqa_strategy_name=to_snake_case_strategy_name(strategy_name=vqa_strategy_type),\n",
    "        country=COUNTRY,\n",
    "        file_type=FILE_TYPE,\n",
    "        prompt_type_name=prompt_type,\n",
    "        question_id=question_id,\n",
    "    )\n",
    "    world_med_qa_v_plot_helpers.visualize_qa_pair_row(\n",
    "        row=row,\n",
    "        image_width=image_width,\n",
    "        model_answer=model_answer\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
